{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is a neural network?\n",
    "\n",
    "_\"...a computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs.\"_\n",
    "\n",
    "In \"Neural Network Primer: Part I\" by Maureen Caudill, AI Expert, Feb. 1989\n",
    "\n",
    "\n",
    "## Here is an example of a simple Neural Network (NN):\n",
    "\n",
    "![ann](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/269px-Artificial_neural_network.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are these processing units (artificial neurons)?\n",
    "\n",
    "An artificial neuron is a mathematical function conceived as a model of biological neurons. The artificial neuron receives one or more inputs (_features_ $X_0$, $X_1$, $X_2$ ...), sums them with different weights ($W_0$, $W_1$, $W_2$ ...) and uses this sum as an argument for a nonlinear function ( $f$ also called _activation function_). \n",
    "\n",
    "$X_0$ typically equals to 1, and it is called _bias_. \n",
    "The weights of each of the neurons are determined when we train the neural network. Initially, these weights are randomly initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are these nonlinear activation functions $f$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We use numpy to do math\n",
    "import numpy as np\n",
    "\n",
    "# We use matplotlib to plot graphics\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# We use pickle to load data\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install keras==2.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This cell won't run without you making some changes\n",
    "\n",
    "# Here is a definition of Rectified linear unit (ReLU) function\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0, x)\n",
    "\n",
    "# Here is a definition of Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# We generate X = [-10, ... 10]\n",
    "X=np.linspace(-10,10, 100)\n",
    "\n",
    "# We calculate the output of activation functions\n",
    "relu_Y=relu(X.copy())\n",
    "sigmoid_Y=sigmoid(X.copy())\n",
    "\n",
    "# One of the activation functions is a hyperbolic tangent function\n",
    "# This fuction can be found in every mathimatical package, try to use numpy to calculate the tanh values of X \n",
    "\n",
    "tanh_Y = np.tanh(X.copy())# <---- calculate tanh of X here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We plot these functions\n",
    "plt.figure(figsize=(20,6))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title('Tanh')\n",
    "plt.plot(X,tanh_Y)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Rectified linear unit (ReLU)\")\n",
    "plt.plot(X,relu_Y)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Sigmoid\")\n",
    "plt.plot(X,sigmoid_Y)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's go through a demo of a NN:\n",
    "Execute the following cell to run the demo. There is a play button to train the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame; \n",
    "IFrame('http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.85093&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&learningRate_hide=true&regularizationRate_hide=true&percTrainData_hide=true&noise_hide=true&regularization_hide=true&problem_hide=true',width=1020, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to deal with images?\n",
    "\n",
    "Mathematically images can be represented as matrices of data. An image consists of pixels and each element of matrix stores an information about one pixel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## RGB channels\n",
    "\n",
    "For human vision it's enough to have three colors to represent the visible spectrum: Red (R), Green (G) and Blue (B) â€” also called _channels_.\n",
    "Intensity of each color can be represented either by 1 bit [0-1] or 8 bits [0-255]. Different colors are the result of mixing R, G and B channels with different intensities.\n",
    "\n",
    "Some images can have more than 3 channels - for example satellites take pictures in Infrared radiation (IR) spectrum and have additional IR, Near IR channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:14:53.470463Z",
     "start_time": "2018-04-06T11:14:52.282396Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "img=matplotlib.image.imread('rgb.png')\n",
    "\n",
    "plt.figure(figsize=(20,6));\n",
    "plt.subplot(1,4,1);\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.subplot(1,4,2);\n",
    "plt.imshow(img[...,0], cmap='Reds')\n",
    "\n",
    "plt.subplot(1,4,3);\n",
    "plt.imshow(img[...,1], cmap='Greens')\n",
    "\n",
    "plt.subplot(1,4,4);\n",
    "plt.imshow(img[...,2], cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do artificial neurons deal with pixels?\n",
    "\n",
    "In the demo above we were using a 2D dataset and initially we used features such as coordinates of the points. \n",
    "\n",
    "For images we initially have intensities of 3 channels, and instead of fully connected neurons we will use neurons based on mathematical operation called _convolution_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutions\n",
    "\n",
    "For the image case a convolution can be simplified as a simple sliding operaton of a matrix (called _kernel_ or _filter_) over with the values of the image channel (another matrix).\n",
    "Let's imagine we have a kernel (in red) and pixel values (in green):\n",
    "\n",
    "\\begin{align*}\\color{red}{\\begin{bmatrix}\n",
    "1 & 0 & 1\\\\\n",
    "0 & 1 & 0\\\\\n",
    "1 & 0 & 1\\\\\n",
    "\\end{bmatrix}} * \n",
    "\\color{green}{\\begin{bmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "0 & 1 & 1\\\\\n",
    "0 & 0 & 1\\\\\n",
    "\\end{bmatrix}} =\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "=1\\cdot 1 + 0\\cdot 1 + 1\\cdot 1 + 0\\cdot 0 + 1\\cdot 1+ 0\\cdot 1 + 0\\cdot 1 + 0\\cdot 0 + 1\\cdot 1 = 4\n",
    "\\end{align*}\n",
    "The resulted feature is smaller than the original image, so in order to keep the same size, we can pad the original images with 0 from every side. This is called _padding_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Now we slide this kernel/filter along the image\n",
    "Below you can see an example of multiplying a kernel  (red numbers in the corners of the yellow matrix) with some values of the image (green). This results in a new matrix (red) which we will use as a new feature in our NN. The sliding window step is called a _stride_.\n",
    "![Convolution](http://deeplearning.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif)\n",
    "[Source](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An example of an edge detector convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:14:50.363152Z",
     "start_time": "2018-04-06T11:14:50.360652Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We use scipy to do convolutions\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:14:53.757224Z",
     "start_time": "2018-04-06T11:14:53.471997Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "img=matplotlib.image.imread('cat.jpg')\n",
    "\n",
    "# A special kernel used for edge detection\n",
    "k = [\n",
    "    [-1, -1, -1],\n",
    "    [-1,  8, -1],\n",
    "    [-1, -1, -1]\n",
    "    ]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# We plot a red channel of the original image\n",
    "plt.title('original image')\n",
    "plt.axis('off')\n",
    "plt.imshow(img[...,0],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:14:53.757224Z",
     "start_time": "2018-04-06T11:14:53.471997Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We calculate a new feature appying the kernel k to the red channel\n",
    "new_feature = signal.convolve2d(img[...,0], k, boundary='symm', mode='same')\n",
    "\n",
    "# We plot the new feature\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('edge feature image')\n",
    "plt.axis('off')\n",
    "plt.imshow(new_feature,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Play around with different convolutions\n",
    "\n",
    "Check the wikipedia Kernel page [https://en.wikipedia.org/wiki/Kernel_(image_processing)](https://bit.ly/2yfaapD) to find different convolutions used in image processing. Try yourself how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Max pooling\n",
    "\n",
    "Another mathematical operation used in NN is _Max pooling_. Basically, we select a largest number from a part of the matrix. This is used to reduce the size of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Max pooling](https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png)\n",
    "[Source](https://en.wikipedia.org/wiki/Convolutional_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training the neural network\n",
    "\n",
    "It's important to understand that when we define the neural network architecture we only code how many layers we want it to have (number of hidden layers), what type of layer it is going to be (convolutional, max pooling, drop out, ...). We don't define the kernels/filters that will be used. Those filters are learned by NN during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Simple Neural Network\n",
    "Let's see how a neural network generate features for the simple black and white image. Please execute the following cell to run the demo or follow the link [https://transcranial.github.io/keras-js/#/mnist-cnn](https://transcranial.github.io/keras-js/#/mnist-cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://transcranial.github.io/keras-js/#/mnist-cnn', width=900, height=3100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MNIST and Fashion MNIST\n",
    "\n",
    "_MNIST_ is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
    "\n",
    "![mnist](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "It was used to train the second demo, that was shown above. For this workshop we will use another public dataset [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist), that has similar 28 pixels x 28 pixels grayscale images of 10 clases.\n",
    "\n",
    "![fashion_mnist](https://4.bp.blogspot.com/-OQZGt_5WqDo/Wa_Dfa4U15I/AAAAAAAAAUI/veRmAmUUKFA19dVw6XCOV2YLO6n-y_omwCLcBGAs/s1600/out.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:14:56.283651Z",
     "start_time": "2018-04-06T11:14:55.609794Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We use Fashion mnist dataset\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# We download and load the data\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:14:56.724786Z",
     "start_time": "2018-04-06T11:14:56.720936Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Train dataset size {0}\".format(x_train.shape))\n",
    "print(\"Test dataset size {0}\".format(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We have 60000 train images and 10000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:14:59.153939Z",
     "start_time": "2018-04-06T11:14:59.023807Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take 4 random images, by generating 4 random indices and reshaping it\n",
    "random_images=x_train[np.random.choice(60000,4)].reshape(4,28,28)\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(random_images[0], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(random_images[1], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(random_images[2], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(random_images[3], cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# You can explore the Fashion dataset yourself\n",
    "Try to show some random images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Let's make a neural network image search engine\n",
    "\n",
    "Our goal is to build a simple image search engine, that will find images similar to the requested one.\n",
    "We will have the training part of the Fashion MNIST as our dataset of known images. Then we will take random images from the test dataset, which our image search have never seen before, to find similar images in the train dataset.\n",
    "\n",
    "```\n",
    "Images in the train dataset: ðŸ‘š ðŸ‘ž ðŸ‘– ðŸ‘œ ...\n",
    "Images in the test dataset: ðŸ‘¢ ðŸ‘™ ðŸ‘• ðŸ‘Ÿ ...\n",
    "\n",
    "User wants to find an image most similar to ðŸ‘Ÿ. \n",
    "The search engine will return ðŸ‘ž.\n",
    "```\n",
    "\n",
    "Under the hood of our search engine we will have a neural network that will take an image as input and give a vector as an output (it will encode the image into a vector). Something like this:\n",
    "\n",
    "\\begin{align*}\n",
    "NN_{encoder}(ðŸ‘š) = [3.5, 1.2, 4.3, ... 1.1, -0.9]\\\\\n",
    "NN_{encoder}(ðŸ‘ž) = [0.4, -1.3, 5.6, ... 2.0, 7.5]\\\\\n",
    "NN_{encoder}(ðŸ‘•) = [1.2, -0.3, 1.1, ... -0.4, 1.3]\\\\\n",
    "NN_{encoder}(ðŸ‘Ÿ) = [0.3, -1.1, 4.1, ... 1.1, 6.3]\n",
    "\\end{align*}\n",
    "\n",
    "So basically, we will have a short numeric representation of the image.\n",
    "\n",
    "To find similar images we will compare two output vectors **q** and **p** and those vectors that will be the closest to each other (having the minimal _euclidean distance_) will represent the most similar images. \n",
    "\n",
    "\\begin{align} \n",
    "distance_{euclidean}(\\mathbf{q},\\mathbf{p}) = d(\\mathbf{q},\\mathbf{p}) & = \\sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \\cdots + (q_n-p_n)^2} = \\sqrt{\\sum_{i=1}^n (q_i-p_i)^2}\\end{align}\n",
    "\n",
    "For example, if we compare ðŸ‘š and ðŸ‘Ÿ we got a distance of 95.7:\n",
    "\n",
    "\\begin{align*}\n",
    "d(NN_{encoder}(ðŸ‘š), NN_{encoder}(ðŸ‘Ÿ)) = \\sqrt{(3.5-0.3)^2 + (1.2+1.1)^2 + \\cdots + (-0.9-6.3)^2} = 95.7 \n",
    "\\end{align*}\n",
    "\n",
    "But if we compare ðŸ‘ž and ðŸ‘Ÿ we will get much closer distance 12.6:\n",
    "\n",
    "\\begin{align*}\n",
    "d(NN_{encoder}(ðŸ‘ž), NN_{encoder}(ðŸ‘Ÿ)) = \\sqrt{(0.4-0.3)^2 + (-1.3-1.1)^2 + \\cdots + (7.5-6.3)^2} = 12.6 \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The output vectors of ðŸ‘ž and ðŸ‘Ÿ are still different, but they are much closer than ðŸ‘š and ðŸ‘Ÿ. So the search engine will return the ðŸ‘ž for the request ðŸ‘Ÿ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The goal of our NN is to calculate these vectors. How can we do that?\n",
    "\n",
    "For these purpose we will build two NN.\n",
    "\n",
    "* Firstly, we will use an auto-encoder NN architecture. It tries to encode-decode the images. It takes an image as input and tries to encode it to some $\\color{cyan}{\\text{vector}}$ (that's the $\\color{LightSteelBlue}{\\text{encoding part}}$), and then it tries to reconstruct the image from the vector (that's the $\\color{LightPink}{\\text{decoding part}}$).\n",
    "\n",
    "   ![autoencoder](https://skymind.ai/images/wiki/deep_autoencoder.png)\n",
    "\n",
    "   In our pseudomath notation, this can be represented as:\n",
    "\n",
    "\\begin{align}\n",
    "&NN_{autoencoder}(ðŸ‘š) = ðŸ‘š\\\\ \n",
    "\\\\\n",
    "&ðŸ‘š \\xrightarrow{encoding} [0.4, -1.3, 5.6, \\ldots 2.0, 7.5] \\xrightarrow{decoding} ðŸ‘š \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "&NN_{autoencoder}(ðŸ‘ž) = ðŸ‘ž\\\\ \n",
    "\\\\\n",
    "&ðŸ‘ž \\xrightarrow{encoding} [3.5, 1.2, 4.3, \\ldots 1.1, -0.9] \\xrightarrow{decoding} ðŸ‘ž\n",
    "\\end{align}\n",
    "\n",
    "------\n",
    "\n",
    "* Secondly, we will use only the $\\color{LightSteelBlue}{\\text{encoding part}}$ of our $NN_{autoencoder}(X)$ as our $NN_{encoder}(X)$ for the search engine, deleting the decoder part:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "NN_{encoder}(ðŸ‘š) = [0.4, -1.3, 5.6, \\ldots 2.0, 7.5]\\\\\n",
    "NN_{encoder}(ðŸ‘ž) = [3.5, 1.2, 4.3, \\ldots 1.1, -0.9]\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We use Keras framework to build Neural networks\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's code the encoder part\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "ðŸ‘š \\xrightarrow{encoding} [128\\text{-dimentional vector}]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:14:59.986578Z",
     "start_time": "2018-04-06T11:14:59.810008Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# This cell won't run without you making some changes\n",
    "\n",
    "input_img = Input(shape=(28, 28, 1)) # This will be the input ðŸ‘š\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded_feature_vector = MaxPooling2D((2, 2), padding='same', name='')(x) #  <---- name your feature vector somehow\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional compressed feature vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's code the decoder part\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "[128\\text{-dimentional vector}] \\xrightarrow{decoding} ðŸ‘š\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:14:59.986578Z",
     "start_time": "2018-04-06T11:14:59.810008Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded_feature_vector)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded_output = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's combine encoder and decoder parts into an autoencoder model\n",
    "\n",
    "\\begin{align}\n",
    "&NN_{autoencoder}(ðŸ‘š) = ðŸ‘š\\\\ \n",
    "\\\\\n",
    "&ðŸ‘š \\xrightarrow{encoding} [128\\text{-dimentional vector}] \\xrightarrow{decoding} ðŸ‘š \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "&NN_{autoencoder}(ðŸ‘ž) = ðŸ‘ž\\\\ \n",
    "\\\\\n",
    "&ðŸ‘ž \\xrightarrow{encoding} [128\\text{-dimentional vector}] \\xrightarrow{decoding} ðŸ‘ž\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:14:59.986578Z",
     "start_time": "2018-04-06T11:14:59.810008Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# The first model is autoencoder model, it takes the input image and results in a decoded image\n",
    "autoencoder_model = Model(input_img, decoded_output)\n",
    "# Compile the first model\n",
    "autoencoder_model.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(autoencoder_model, show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's take only the first part for the encoder model\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "&NN_{encoder}(ðŸ‘š) = [128\\text{-dimentional vector}]\\\\ \n",
    "\\\\\n",
    "&ðŸ‘š \\xrightarrow{encoding} [128\\text{-dimentional vector}]\\\\\n",
    "\\\\\n",
    "\\\\\n",
    "&NN_{encoder}(ðŸ‘ž) = [128\\text{-dimentional vector}]\\\\ \n",
    "\\\\\n",
    "&ðŸ‘ž \\xrightarrow{encoding} [128\\text{-dimentional vector}]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:14:59.986578Z",
     "start_time": "2018-04-06T11:14:59.810008Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# The second NN model is only a half of the first model, it take the input image and gives the encoded vector as output\n",
    "encoder_model = Model(inputs=autoencoder_model.input,\n",
    "                                 outputs=autoencoder_model.get_layer('').output) # <---- take the output from the feature vector\n",
    "# Compile the second model\n",
    "encoder_model.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(encoder_model, show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:15:06.345364Z",
     "start_time": "2018-04-06T11:15:06.175144Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This cell won't run without you making some changes\n",
    "\n",
    "# We need to scale the image from [0-255] to [0-1] for better performance of activation functions\n",
    "# Please normalize the datasets\n",
    "\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:15:06.345364Z",
     "start_time": "2018-04-06T11:15:06.175144Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We train the NN in batches (groups of images), so we reshape the dataset\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "\n",
    "print(\"Train dataset size is {0}\".format(x_train.shape))\n",
    "print(\"Test dataset size is {0}\".format(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's train the neural network\n",
    "\n",
    "Here we will train our NN. It will learn the optimal weights used in the convolutional layers by itself.\n",
    "Both `x` and `y` in the `fit` function equal to `x_train`, because our autoencoder wants to have the output equal to input $NN_{autoencoder}(ðŸ‘š) = ðŸ‘š$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It takes 10 minutes to train this neural network \n",
    "# If you want to skip the training process, please load the weights directly in the next cell\n",
    "\n",
    "learning_history=autoencoder_model.fit(x=x_train, y=x_train, epochs=10, batch_size=128, \n",
    "                                 shuffle=True, validation_data=(x_test, x_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:15:03.929825Z",
     "start_time": "2018-04-06T11:15:03.783248Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "autoencoder_model.load_weights('autoencoder_0.2925.weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's see how it performs\n",
    "\n",
    "In our pseudomath notation, this can be represented as:\n",
    "\n",
    "\\begin{align}\n",
    "&NN_{autoencoder}(ðŸ‘š) = ðŸ‘š\\\\ \n",
    "\\\\\n",
    "&ðŸ‘š \\xrightarrow{encoding} [128\\text{-dimentional vector}] \\xrightarrow{decoding} ðŸ‘š \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "&NN_{autoencoder}(ðŸ‘ž) = ðŸ‘ž\\\\ \n",
    "\\\\\n",
    "&ðŸ‘ž \\xrightarrow{encoding} [128\\text{-dimentional vector}] \\xrightarrow{decoding} ðŸ‘ž\n",
    "\\end{align}\n",
    "\n",
    "We will try to encode-decode `x_test` part of the dataset, that NN has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T07:06:48.927203Z",
     "start_time": "2018-04-06T07:06:46.640932Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "encoded_decoded_image=autoencoder_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T08:01:14.651347Z",
     "start_time": "2018-04-06T08:01:13.465957Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# we take 5 consecutive images from the test dataset starting from a random index\n",
    "random_index=np.random.randint(0,10000)\n",
    "\n",
    "for i,r in zip(x_test[random_index:random_index+5], encoded_decoded_image[random_index:random_index+5]):\n",
    "    plt.figure()\n",
    "    ax=plt.subplot(1,2,1)\n",
    "    ax.set_title(\"Original:\")\n",
    "    ax.imshow(np.squeeze(i), cmap=plt.get_cmap('gray'))\n",
    "    \n",
    "    ax=plt.subplot(1,2,2)\n",
    "    ax.set_title(\"Encoded-Decoded:\")\n",
    "    ax.imshow(np.squeeze(r), cmap=plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time to encode all the images in the train dataset\n",
    " \n",
    "\\begin{align*}\n",
    "&NN_{encoder}(ðŸ‘š) = [128\\text{-dimentional vector}]\\\\ \n",
    "&NN_{encoder}(ðŸ‘ž) = [128\\text{-dimentional vector}]\\\\ \n",
    "&NN_{encoder}(ðŸ‘–) = [128\\text{-dimentional vector}]\\\\ \n",
    "&NN_{encoder}(ðŸ‘œ) = [128\\text{-dimentional vector}]\\\\ \n",
    "&NN_{encoder}(ðŸ‘ ) = [128\\text{-dimentional vector}]\\\\ \n",
    "&\\cdots\n",
    "\\end{align*}\n",
    "\n",
    "Later we will compare a vector of the requested image with these encoded vectors, to find similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:15:27.459605Z",
     "start_time": "2018-04-06T11:15:22.743259Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x_train_encoded = encoder_model.predict(x_train)\n",
    "print(\"Shape of the encoded dataset: {0}\".format(x_train_encoded.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Let's flatten the dataset\n",
    "We flatten (4, 4, 8) to a flat 128-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T11:15:30.982358Z",
     "start_time": "2018-04-06T11:15:30.979716Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x_train_encoded_128=x_train_encoded.reshape(x_train_encoded.shape[0],128)\n",
    "print(\"Shape of the encoded dataset: {0}\".format(x_train_encoded_128.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plotting 128 dimentional vector is difficult, let's reduce it to 2D and 3D\n",
    "\n",
    "We will use t-SNE algorithm to reduce the dimentions:\n",
    "\n",
    "\\begin{align}\n",
    "&[128\\text{-dimentional vector}] \\xrightarrow{t-SNE} [x, y, z]\\\\\n",
    "&[128\\text{-dimentional vector}] \\xrightarrow{t-SNE} [x, y]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U --no-deps git+https://github.com/DmitryUlyanov/Multicore-TSNE.git#egg=MulticoreTSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T07:11:04.042771Z",
     "start_time": "2018-04-06T07:11:04.040055Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We use t-SNE to shrink 128 dimentional vector in 2D or 3D\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "# We use plotly to plot 3D\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Reducing to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T07:30:14.82363Z",
     "start_time": "2018-04-06T07:30:05.025518Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=3, verbose=1000, n_jobs=2)\n",
    "\n",
    "# Warning, it will take more than 30 min to run the following line in Azure cloud notebook\n",
    "#transformation_3d = tsne.fit_transform(x_train_encoded_128[...,:1000])\n",
    "\n",
    "# We load 3 dimentional t-SNE from the pickle file\n",
    "transformation_3d = pickle.load(open(\"transformation_3d.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T07:24:45.255287Z",
     "start_time": "2018-04-06T07:11:18.829Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Shape of the encoded dataset: {0}\".format(transformation_3d.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting 3D cloud\n",
    "Every dot is an encoded image:\n",
    "\\begin{align}\n",
    "ðŸ‘š \\xrightarrow{NN_{encoder}} [128-\\text{dimentional vector}] \\xrightarrow{t-SNE} [x, y, z]\n",
    "\\end{align}\n",
    "The color of the dot represents the class the image belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T13:03:08.432077Z",
     "start_time": "2018-04-05T13:03:07.975289Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x, y, z = np.rollaxis(transformation_3d, 1, 0)[...,:1000]\n",
    "trace1 = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=plt.cm.tab10(y_train[:1000]),\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Try to reduce the first 1000 examples of 128 dimentional vector to 2d vector\n",
    "# It will take about 3 min 20 sec to do this task on Azure cloud notebook\n",
    "# Don't run tsne on all 60000 samples, since it will take up to 20 min\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1000, n_jobs=2)\n",
    "transformation_2d = tsne.fit_transform(x_train_encoded_128[...,:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_2d = pickle.load(open(\"transformation_2d.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Shape of the encoded dataset: {0}\".format(transformation_2d.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting 2D cloud\n",
    "Every dot is an encoded image:\n",
    "\\begin{align}\n",
    "ðŸ‘š \\xrightarrow{NN_{encoder}} [128-\\text{dimentional vector}] \\xrightarrow{t-SNE} [x, y]\n",
    "\\end{align}\n",
    "The color of the dot represents the class the image belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T14:00:31.922916Z",
     "start_time": "2018-04-23T14:00:31.219289Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,10))\n",
    "plt.scatter(transformation_2d[:1000,0], transformation_2d[:1000,1], marker='.', c=plt.cm.tab10(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Time to build a search engine\n",
    "\n",
    "At this point we have all the images encoded to their 128-dimentional representations:\n",
    "\\begin{align*}\n",
    "&NN_{encoder}(ðŸ‘š) = [128\\text{-dimentional vector}]\\\\ \n",
    "&NN_{encoder}(ðŸ‘ž) = [128\\text{-dimentional vector}]\\\\ \n",
    "&NN_{encoder}(ðŸ‘–) = [128\\text{-dimentional vector}]\\\\ \n",
    "&NN_{encoder}(ðŸ‘œ) = [128\\text{-dimentional vector}]\\\\ \n",
    "&NN_{encoder}(ðŸ‘ ) = [128\\text{-dimentional vector}]\\\\ \n",
    "&\\cdots\n",
    "\\end{align*}\n",
    "\n",
    "Every time we have a new image ðŸ‘Ÿ\n",
    "want to find a similar image, we will encode it to the $[128\\text{-dimentional vector}]$ and compare it to the exsisting dataset. This search and comparison will be done automatically by python `KDTree`. It calculates the Euclidian distances and return the index of the closest vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "# We create KDTree and put all the exsisting vectors in the tree\n",
    "tree = spatial.KDTree(x_train_encoded_128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Now we encode the test dataset\n",
    "\n",
    "\\begin{align*}\n",
    "&NN_{encoder}(ðŸ‘¢ ) = [128\\text{-dimentional vector}]\\\\ \n",
    "&NN_{encoder}(ðŸ‘™) = [128\\text{-dimentional vector}]\\\\ \n",
    "&NN_{encoder}(ðŸ‘•) = [128\\text{-dimentional vector}]\\\\ \n",
    "&NN_{encoder}(ðŸ‘Ÿ) = [128\\text{-dimentional vector}]\\\\ \n",
    "&NN_{encoder}(ðŸ’¼) = [128\\text{-dimentional vector}]\\\\ \n",
    "&\\cdots\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "NN_{encoder}(test) \\rightarrow [\\text{test_result128}]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T13:45:53.769196Z",
     "start_time": "2018-04-05T13:45:53.043663Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Now we encode all the the test dataset\n",
    "test_result = encoder_model.predict(x_test)\n",
    "# Flatten it to 128 dimentions\n",
    "test_result128=test_result.reshape(10000,128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The search process\n",
    "Let's take the random index from the test dataset and pull 5 consecutive images from there (ðŸ‘¢ ðŸ‘™ ðŸ‘• ðŸ‘Ÿ ðŸ’¼).\n",
    "\n",
    "Simultaniously, we will pull 5 vectors which encode these images from test_result128.\n",
    "\n",
    "Using KDTree we will find the closest vector from the train dataset for each of the requested vectors.\n",
    "KDTree will return the index of the image from the train dataset, which has the closest vector to the requested one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T08:05:29.155743Z",
     "start_time": "2018-04-06T08:05:27.019521Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Taking random index\n",
    "random_index=np.random.randint(0,9995)\n",
    "for i,f in zip(x_test[random_index:random_index+5], test_result128[random_index:random_index+5]):\n",
    "    # KDTree returns euclidian distance and the index from the train dataset\n",
    "    distance, index = tree.query(f)\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Requested:\")\n",
    "    plt.imshow(np.squeeze(i), cmap=plt.get_cmap('gray'))\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"Found:\")\n",
    "    plt.imshow(np.squeeze(x_train[index]), cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    print(\"Distance between two 128-dimentional vectors: %f\\n\" % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advanced version\n",
    "For advanced version you can try to DIY encoding of any image with excisting Deep Neural Network.\n",
    "To classify images Deep Neural Networks like VGG16, has many layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We can use the output of the fully connected layer of the pretrained NN as a 4096 dimentional feature vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# do some imports\n",
    "from keras import applications\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# download the model (might take time, since we donwload ~500Mb)\n",
    "base_model = applications.VGG19(weights='imagenet')\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('fc1').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "img = image.load_img('fashion.png', target_size=(224, 224))\n",
    "img = image.img_to_array(img)\n",
    "img = np.expand_dims(img, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "features = model.predict(img)\n",
    "\n",
    "print(\"Feature vector shape: {0}\".format(features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
