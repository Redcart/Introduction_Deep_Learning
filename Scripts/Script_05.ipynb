{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Keras: Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scorde\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a data set composed by 70 000 images of clothes from Zalando:\n",
    "- 60 000 images in the training set\n",
    "- 10 000 images in the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is formed by 28x28=784 pixels encoded in greyscale (0-255). starting from zero (white), the higher the number, the darker is the pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pictures are dirtibuted across 10 different classes:\n",
    "- 0 T-shirt/top\n",
    "- 1 Trouser\n",
    "- 2 Pullover\n",
    "- 3 Dress\n",
    "- 4 Coat\n",
    "- 5 Sandal\n",
    "- 6 Shirt\n",
    "- 7 Sneaker\n",
    "- 8 Bag\n",
    "- 9 Ankle boot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   1   0   0   0   0  41 188 103  54  48  43  87 168\n",
      "  133  16   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0  49 136 219 216 228 236 255 255 255 255 217\n",
      "  215 254 231 160  45   0   0   0   0   0]\n",
      " [  0   0   0   0   0  14 176 222 224 212 203 198 196 200 215 204 202 201\n",
      "  201 201 209 218 224 164   0   0   0   0]\n",
      " [  0   0   0   0   0 188 219 200 198 202 198 199 199 201 196 198 198 200\n",
      "  200 200 200 201 200 225  41   0   0   0]\n",
      " [  0   0   0   0  51 219 199 203 203 212 238 248 250 245 249 246 247 252\n",
      "  248 235 207 203 203 222 140   0   0   0]\n",
      " [  0   0   0   0 116 226 206 204 207 204 101  75  47  73  48  50  45  51\n",
      "   63 113 222 202 206 220 224   0   0   0]\n",
      " [  0   0   0   0 200 222 209 203 215 200   0  70  98   0 103  59  68  71\n",
      "   49   0 219 206 214 210 250  38   0   0]\n",
      " [  0   0   0   0 247 218 212 210 215 214   0 254 243 139 255 174 251 255\n",
      "  205   0 215 217 214 208 220  95   0   0]\n",
      " [  0   0   0  45 226 214 214 215 224 205   0  42  35  60  16  17  12  13\n",
      "   70   0 189 216 212 206 212 156   0   0]\n",
      " [  0   0   0 164 235 214 211 220 216 201  52  71  89  94  83  78  70  76\n",
      "   92  87 206 207 222 213 219 208   0   0]\n",
      " [  0   0   0 106 187 223 237 248 211 198 252 250 248 245 248 252 253 250\n",
      "  252 239 201 212 225 215 193 113   0   0]\n",
      " [  0   0   0   0   0  17  54 159 222 193 208 192 197 200 200 200 200 201\n",
      "  203 195 210 165   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  47 225 192 214 203 206 204 204 205 206 204\n",
      "  212 197 218 107   0   0   0   0   0   0]\n",
      " [  0   0   0   0   1   6   0  46 212 195 212 202 206 205 204 205 206 204\n",
      "  212 200 218  91   0   3   1   0   0   0]\n",
      " [  0   0   0   0   0   1   0  11 197 199 205 202 205 206 204 205 207 204\n",
      "  205 205 218  77   0   5   0   0   0   0]\n",
      " [  0   0   0   0   0   3   0   2 191 198 201 205 206 205 205 206 209 206\n",
      "  199 209 219  74   0   5   0   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0 188 197 200 207 207 204 207 207 210 208\n",
      "  198 207 221  72   0   4   0   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0 215 198 203 206 208 205 207 207 210 208\n",
      "  200 202 222  75   0   4   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 212 198 209 206 209 206 208 207 211 206\n",
      "  205 198 221  80   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 204 201 205 208 207 205 211 205 210 210\n",
      "  209 195 221  96   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 202 201 205 209 207 205 213 206 210 209\n",
      "  210 194 217 105   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 204 204 205 208 207 205 215 207 210 208\n",
      "  211 193 213 115   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 204 207 207 208 206 206 215 210 210 207\n",
      "  212 195 210 118   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 198 208 208 208 204 207 212 212 210 207\n",
      "  211 196 207 121   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 198 210 207 208 206 209 213 212 211 207\n",
      "  210 197 207 124   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 172 210 203 201 199 204 207 205 204 201\n",
      "  205 197 206 127   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 188 221 214 234 236 238 244 244 244 240\n",
      "  243 214 224 162   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 139 146 130 135 135 137 125 124 125 121\n",
      "  119 114 130  76   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[1, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[1000], cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit ici l'image d'un pantalon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(y_train[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEGtJREFUeJzt3G+snnV9x/H3R+pfnLbIgbC2rhgbFZcg5ATqSMxGTSloLA8kqdmkIV36pGO4mDjwCRlIoskiSjJJGqgrjokENTSOiE3BLHsAUoShUEk7ZPSsSI8roBtRh3734PyqN3Dacx84ve/K7/1KTu7r+l7f675+v7Q5n3P9ue9UFZKk/rxm3AOQJI2HASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnq1KJxD+BITjzxxFqxYsW4hyFJv1fuv//+n1bVxFx9x3QArFixgl27do17GJL0eyXJfw7T5yUgSeqUASBJnTIAJKlTBoAkdcoAkKRODRUASRYnuS3Jj5LsTvL+JCck2ZFkT3td0nqT5Loke5M8lOTMgffZ0Pr3JNlwtCYlSZrbsGcAXwS+XVXvBk4HdgOXAzuraiWws60DnA+sbD+bgOsBkpwAXAmcDZwFXHkoNCRJozdnACR5C/AB4EaAqvpVVT0DrAO2tbZtwIVteR1wU824B1ic5BTgPGBHVR2sqqeBHcDaBZ2NJGlow5wBvAOYBr6c5IEkNyQ5Hji5qp4EaK8ntf6lwL6B/ada7XB1SdIYDPNJ4EXAmcClVXVvki/yu8s9s8kstTpC/YU7J5uYuXTE29/+9iGGd3grLv+XV7T/XB7/7Ie6PPaRju+xPbbHPrrHXkjDnAFMAVNVdW9bv42ZQHiqXdqhvR4Y6F8+sP8yYP8R6i9QVVuqarKqJicm5vwqC0nSyzRnAFTVT4B9Sd7VSquBR4DtwKEneTYAt7fl7cDF7WmgVcCz7RLRncCaJEvazd81rSZJGoNhvwzuUuDmJK8DHgMuYSY8bk2yEXgCuKj13gFcAOwFnmu9VNXBJFcD97W+q6rq4ILMQpI0b0MFQFU9CEzOsmn1LL0FbD7M+2wFts5ngJKko8NPAktSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASerUUAGQ5PEkP0jyYJJdrXZCkh1J9rTXJa2eJNcl2ZvkoSRnDrzPhta/J8mGozMlSdIw5nMG8GdV9b6qmmzrlwM7q2olsLOtA5wPrGw/m4DrYSYwgCuBs4GzgCsPhYYkafReySWgdcC2trwNuHCgflPNuAdYnOQU4DxgR1UdrKqngR3A2ldwfEnSKzBsABTwnST3J9nUaidX1ZMA7fWkVl8K7BvYd6rVDld/gSSbkuxKsmt6enr4mUiS5mXRkH3nVNX+JCcBO5L86Ai9maVWR6i/sFC1BdgCMDk5+ZLtkqSFMdQZQFXtb68HgG8ycw3/qXZph/Z6oLVPAcsHdl8G7D9CXZI0BnMGQJLjk/zBoWVgDfBDYDtw6EmeDcDtbXk7cHF7GmgV8Gy7RHQnsCbJknbzd02rSZLGYJhLQCcD30xyqP+fq+rbSe4Dbk2yEXgCuKj13wFcAOwFngMuAaiqg0muBu5rfVdV1cEFm4kkaV7mDICqegw4fZb6fwOrZ6kXsPkw77UV2Dr/YUqSFpqfBJakThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkTg0dAEmOS/JAkm+19VOT3JtkT5KvJXldq7++re9t21cMvMcVrf5okvMWejKSpOHN5wzgMmD3wPrngGuraiXwNLCx1TcCT1fVO4FrWx9JTgPWA+8F1gJfSnLcKxu+JOnlGioAkiwDPgTc0NYDnAvc1lq2ARe25XVtnbZ9detfB9xSVb+sqh8De4GzFmISkqT5G/YM4AvAp4DftPW3Ac9U1fNtfQpY2paXAvsA2vZnW/9v67PsI0kasTkDIMmHgQNVdf9geZbWmmPbkfYZPN6mJLuS7Jqenp5reJKkl2mYM4BzgI8keRy4hZlLP18AFidZ1HqWAfvb8hSwHKBtfytwcLA+yz6/VVVbqmqyqiYnJibmPSFJ0nDmDICquqKqllXVCmZu4t5VVX8O3A18tLVtAG5vy9vbOm37XVVVrb6+PSV0KrAS+N6CzUSSNC+L5m45rL8FbknyGeAB4MZWvxH4SpK9zPzlvx6gqh5OcivwCPA8sLmqfv0Kji9JegXmFQBV9V3gu235MWZ5iqeqfgFcdJj9rwGume8gJUkLz08CS1KnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6tScAZDkDUm+l+Tfkzyc5O9a/dQk9ybZk+RrSV7X6q9v63vb9hUD73VFqz+a5LyjNSlJ0tyGOQP4JXBuVZ0OvA9Ym2QV8Dng2qpaCTwNbGz9G4Gnq+qdwLWtjySnAeuB9wJrgS8lOW4hJyNJGt6cAVAz/qetvrb9FHAucFurbwMubMvr2jpt++okafVbquqXVfVjYC9w1oLMQpI0b0PdA0hyXJIHgQPADuA/gGeq6vnWMgUsbctLgX0AbfuzwNsG67PsM3isTUl2Jdk1PT09/xlJkoYyVABU1a+r6n3AMmb+an/PbG3tNYfZdrj6i4+1paomq2pyYmJimOFJkl6GeT0FVFXPAN8FVgGLkyxqm5YB+9vyFLAcoG1/K3BwsD7LPpKkERvmKaCJJIvb8huBDwK7gbuBj7a2DcDtbXl7W6dtv6uqqtXXt6eETgVWAt9bqIlIkuZn0dwtnAJsa0/svAa4taq+leQR4JYknwEeAG5s/TcCX0myl5m//NcDVNXDSW4FHgGeBzZX1a8XdjqSpGHNGQBV9RBwxiz1x5jlKZ6q+gVw0WHe6xrgmvkPU5K00PwksCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1as4ASLI8yd1Jdid5OMllrX5Ckh1J9rTXJa2eJNcl2ZvkoSRnDrzXhta/J8mGozctSdJchjkDeB74ZFW9B1gFbE5yGnA5sLOqVgI72zrA+cDK9rMJuB5mAgO4EjgbOAu48lBoSJJGb84AqKonq+r7bfnnwG5gKbAO2NbatgEXtuV1wE014x5gcZJTgPOAHVV1sKqeBnYAaxd0NpKkoc3rHkCSFcAZwL3AyVX1JMyEBHBSa1sK7BvYbarVDleXJI3B0AGQ5M3A14FPVNXPjtQ6S62OUH/xcTYl2ZVk1/T09LDDkyTN01ABkOS1zPzyv7mqvtHKT7VLO7TXA60+BSwf2H0ZsP8I9Reoqi1VNVlVkxMTE/OZiyRpHoZ5CijAjcDuqvr8wKbtwKEneTYAtw/UL25PA60Cnm2XiO4E1iRZ0m7+rmk1SdIYLBqi5xzg48APkjzYap8GPgvcmmQj8ARwUdt2B3ABsBd4DrgEoKoOJrkauK/1XVVVBxdkFpKkeZszAKrq35j9+j3A6ln6C9h8mPfaCmydzwAlSUeHnwSWpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUqTkDIMnWJAeS/HCgdkKSHUn2tNclrZ4k1yXZm+ShJGcO7LOh9e9JsuHoTEeSNKxhzgD+EVj7otrlwM6qWgnsbOsA5wMr288m4HqYCQzgSuBs4CzgykOhIUkajzkDoKr+FTj4ovI6YFtb3gZcOFC/qWbcAyxOcgpwHrCjqg5W1dPADl4aKpKkEXq59wBOrqonAdrrSa2+FNg30DfVaoerv0SSTUl2Jdk1PT39MocnSZrLQt8Eziy1OkL9pcWqLVU1WVWTExMTCzo4SdLvvNwAeKpd2qG9Hmj1KWD5QN8yYP8R6pKkMXm5AbAdOPQkzwbg9oH6xe1poFXAs+0S0Z3AmiRL2s3fNa0mSRqTRXM1JPkq8KfAiUmmmHma57PArUk2Ak8AF7X2O4ALgL3Ac8AlAFV1MMnVwH2t76qqevGNZUnSCM0ZAFX1scNsWj1LbwGbD/M+W4Gt8xqdJOmo8ZPAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktSpkQdAkrVJHk2yN8nloz6+JGnGSAMgyXHAPwDnA6cBH0ty2ijHIEmaMeozgLOAvVX1WFX9CrgFWDfiMUiSGH0ALAX2DaxPtZokacRSVaM7WHIRcF5V/WVb/zhwVlVdOtCzCdjUVt8FPDqyAcKJwE9HeLxjhfPui/N+9fujqpqYq2nRKEYyYApYPrC+DNg/2FBVW4AtoxzUIUl2VdXkOI49Ts67L85bh4z6EtB9wMokpyZ5HbAe2D7iMUiSGPEZQFU9n+SvgDuB44CtVfXwKMcgSZox6ktAVNUdwB2jPu6QxnLp6RjgvPvivAWM+CawJOnY4VdBSFKnDAD6/XqKJMuT3J1kd5KHk1w27jGNUpLjkjyQ5FvjHsuoJFmc5LYkP2r/7u8f95hGIcnftP/jP0zy1SRvGPeYjgXdB0DnX0/xPPDJqnoPsArY3NHcAS4Ddo97ECP2ReDbVfVu4HQ6mH+SpcBfA5NV9cfMPICyfryjOjZ0HwB0/PUUVfVkVX2/Lf+cmV8GXXwyO8ky4EPADeMey6gkeQvwAeBGgKr6VVU9M95Rjcwi4I1JFgFv4kWfP+qVAeDXUwCQZAVwBnDveEcyMl8APgX8ZtwDGaF3ANPAl9ulrxuSHD/uQR1tVfVfwN8DTwBPAs9W1XfGO6pjgwEAmaXW1aNRSd4MfB34RFX9bNzjOdqSfBg4UFX3j3ssI7YIOBO4vqrOAP4XeNXf80qyhJmz+lOBPwSOT/IX4x3VscEAGOLrKV7NkryWmV/+N1fVN8Y9nhE5B/hIkseZueR3bpJ/Gu+QRmIKmKqqQ2d5tzETCK92HwR+XFXTVfV/wDeAPxnzmI4JBkDHX0+RJMxcD95dVZ8f93hGpaquqKplVbWCmX/vu6rqVf8XYVX9BNiX5F2ttBp4ZIxDGpUngFVJ3tT+z6+mg5vfwxj5J4GPNZ1/PcU5wMeBHyR5sNU+3T6trVenS4Gb2x87jwGXjHk8R11V3ZvkNuD7zDz59gB+Khjwk8CS1C0vAUlSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI69f9HBJoKK7J96gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.bar(np.unique(y_train), np.bincount(y_train))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cloth appears 6 000 in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = x_train / 255\n",
    "x_test_scaled = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled_reshape = x_train_scaled.reshape((60000, 28, 28, 1))\n",
    "x_test_scaled_reshape = x_test_scaled.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN) ==> VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Import of libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras.activations import relu, sigmoid, softmax\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.initializers import glorot_uniform\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Construction of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import relu, softmax\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "nb_classes=len(np.unique(y_train))\n",
    "nb_height=x_train_scaled_reshape.shape[1]\n",
    "nb_width=x_train_scaled_reshape.shape[2]\n",
    "nb_channels=x_train_scaled_reshape.shape[3]\n",
    "\n",
    "cnn_clothes=Sequential()\n",
    "\n",
    "cnn_clothes.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation=relu, input_shape=(nb_height, nb_width, nb_channels)))\n",
    "cnn_clothes.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation=relu))\n",
    "\n",
    "cnn_clothes.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn_clothes.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation=relu))\n",
    "cnn_clothes.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation=relu))\n",
    "\n",
    "cnn_clothes.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn_clothes.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation=relu))\n",
    "cnn_clothes.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation=relu))\n",
    "cnn_clothes.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation=relu))\n",
    "\n",
    "cnn_clothes.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn_clothes.add(Conv2D(filters=512, kernel_size=(3,3), padding='same', activation=relu))\n",
    "cnn_clothes.add(Conv2D(filters=512, kernel_size=(3,3), padding='same', activation=relu))\n",
    "cnn_clothes.add(Conv2D(filters=512, kernel_size=(3,3), padding='same', activation=relu))\n",
    "\n",
    "cnn_clothes.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn_clothes.add(Conv2D(filters=512, kernel_size=(3,3), padding='same', activation=relu))\n",
    "cnn_clothes.add(Conv2D(filters=512, kernel_size=(3,3), padding='same', activation=relu))\n",
    "cnn_clothes.add(Conv2D(filters=512, kernel_size=(3,3), padding='same', activation=relu))\n",
    "\n",
    "#cnn_clothes.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn_clothes.add(Flatten())\n",
    "cnn_clothes.add(Dense(units=4096, activation=relu))\n",
    "cnn_clothes.add(Dense(units=4096, activation=relu))\n",
    "                \n",
    "cnn_clothes.add(Dense(units=nb_classes, activation=softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 7, 7, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 7, 7, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 3, 3, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 1, 1, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 1, 1, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 1, 1, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 33,637,066\n",
      "Trainable params: 33,637,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_clothes.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Compilation of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_clothes.compile(optimizer='adam', \n",
    "                    loss='sparse_categorical_crossentropy', \n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Fitting of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5693s 95ms/step - loss: 9.5018 - acc: 0.1142\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4626s 77ms/step - loss: 14.5063 - acc: 0.1000\n",
      "Epoch 3/10\n",
      "39936/60000 [==================>...........] - ETA: 24:24 - loss: 14.4807 - acc: 0.1016"
     ]
    }
   ],
   "source": [
    "cnn_clothes.fit(x_train_scaled_reshape, y_train,  batch_size=512, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cnn = cnn_clothes.predict_classes(x_test_scaled_reshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "metric_cnn = accuracy_score(y_test, predictions_cnn)\n",
    "print(metric_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We save the model structure in the yaml format (layers, activation functions, dimensions, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml\n",
    "\n",
    "### On sauve la strcture du modèle (couches, fonctions d'activations, dimensions, ...)\n",
    "model_yaml = cnn_clothes.to_yaml()\n",
    "with open(\"cnn_clothes.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We save the set of weights of the model in the h5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We save the 84 600 weights\n",
    "cnn_clothes.save_weights(\"cnn_clothes.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = open('cnn_clothes.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"cnn_clothes.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in mlp_clothes.layers:\n",
    "    g=layer.get_config()\n",
    "    h=layer.get_weights()\n",
    "    print (g)\n",
    "    print (h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
